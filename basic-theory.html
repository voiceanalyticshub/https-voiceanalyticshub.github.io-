<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Voice Analytics Hub - Theoretical Foundations</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Theoretical Foundations</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">The Voice Analytics Hub</a> 
        <div class="sidebar-tools-main">
    <a href="https://www.ibt.unisg.ch/" title="" class="sidebar-tool px-1"><i class="bi bi-globe2"></i></a>
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Home</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Basics</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basic-theory.html" class="sidebar-item-text sidebar-link active">Theoretical Foundations</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./voice-analytics-pipeline.html" class="sidebar-item-text sidebar-link">The Voice Analytics Pipeline</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basic-tutorial.html" class="sidebar-item-text sidebar-link">Voice Analytics in Action</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basic-resources.html" class="sidebar-item-text sidebar-link">Resources to learn more</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./voiceR.html" class="sidebar-item-text sidebar-link">voiceR: Automating voice Analytics</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./about.html" class="sidebar-item-text sidebar-link">About</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Theoretical Foundations</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<p>Voice analytics is a multidisciplinary exploration of human speech, seamlessly integrating phonetics, acoustics, and cognitive psychology. This cutting-edge field unravels the intricacies of speech formation, transmission, and perception, offering profound insights into human communication.</p>
<p>Before diving into the world of voice analytics, it’s essential to grasp the fundamental principles of acoustics. While acoustics is a vast field, we’ll cover some key concepts to provide a foundational understanding of this fascinating domain. These insights will not only enhance your comprehension of voice analytics but also empower you to better interpret your analysis results.</p>
<section id="fundamental-principles-of-acoustics" class="level2">
<h2 class="anchored" data-anchor-id="fundamental-principles-of-acoustics">Fundamental Principles of Acoustics</h2>
<section id="sound-as-wave-in-motion" class="level3">
<h3 class="anchored" data-anchor-id="sound-as-wave-in-motion"><strong>Sound as Wave in Motion</strong></h3>
<p>Sound, at its core, is a wave in motion. These sound waves travel through various elastic media, including air, steel, and water, inducing changes in pressure and motion. Elastic media have the remarkable ability to reshape themselves when subjected to external forces.</p>
</section>
<section id="sound-as-a-longitudinal-wave" class="level3">
<h3 class="anchored" data-anchor-id="sound-as-a-longitudinal-wave"><strong>Sound as a longitudinal Wave</strong></h3>
<p>Waves can be classified based on the orientation of their motion, giving rise to three fundamental types: transverse, longitudinal, and surface waves. In the case of an ideal fluid medium like air and other gases, sound propagates as longitudinal waves. These are waves in which particles oscillate parallel to the direction of the wave’s propagation.</p>
</section>
<section id="complex-periodic-sounds" class="level3">
<h3 class="anchored" data-anchor-id="complex-periodic-sounds"><strong>Complex Periodic Sounds</strong></h3>
<p>The human voice exhibits two distinctive features: periodicity, which involves the consistent repetition of sound patterns, and complexity, marked by the presence of numerous signal components known as harmonic frequencies. Periodicity pertains to the regular recurrence of sound patterns, while complexity highlights the existence of these multiple harmonic frequencies.</p>
</section>
<section id="harmonics-and-frequency" class="level3">
<h3 class="anchored" data-anchor-id="harmonics-and-frequency"><strong>Harmonics and Frequency</strong></h3>
<p>Harmonics are multiples of the fundamental frequency and contribute to the richness and quality of a sound. Frequency measures the number of oscillations occurring per second and is quantified in hertz (Hz), while amplitude denotes the power or loudness of a sound, measured in decibels (dB). Humans can typically hear sounds ranging from 0 to 140 dB, with 0 dB representing the hearing threshold for the human ear. Jitter and shimmer measurements capture frequency and amplitude instability, respectively, making them valuable tools in describing vocal characteristics, speaker evaluation, stress and emotion classification, and vocal pathology detection.</p>
</section>
<section id="delving-into-the-mechanics-of-human-speech" class="level3">
<h3 class="anchored" data-anchor-id="delving-into-the-mechanics-of-human-speech">Delving into the Mechanics of Human Speech</h3>
<p>As previously said, voice is nothing else than a complex periodic sound. However, it involves a complex series of events. Imagine uttering a simple greeting like “Hello” to a friend. To do so, your brain initiates the release of air from your lungs, causing your vocal folds to vibrate. The distinctive sound of your “Hello” is a product of the precise configuration of your articulatory organs, including your teeth, tongue, oral and nasal cavities. These unique vocal signatures are influenced by factors such as gender and body size.</p>
<p>For instance, women typically have higher-pitched voices due to shorter vocal fold lengths, while larger individuals may possess lower-pitched voices due to longer vocal tracts. These vocal variations carry evolutionary implications, shaping perceptions of attractiveness, dominance, and social status.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="data/Theory/production-to-transmission-process.JPG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">From Speech Production and Soundwave Transmission to Perception. Source: <a href="https://www.sciencedirect.com/science/article/pii/S0148296320306044#b0245">Voice analytics in business research</a></figcaption><p></p>
</figure>
</div>
</section>
<section id="recording-sound-waves" class="level3">
<h3 class="anchored" data-anchor-id="recording-sound-waves"><strong>Recording Sound Waves</strong></h3>
<p>Sound waves can be captured and recorded, resulting in an audio signal. During the recording process, audio is converted from its analog form, where it exists as continuous or semi-continuous changes in the signal, into digital format. This conversion is essential for saving audio as digital files. Digital audio quality is determined by the sample rate, which defines the number of “snapshot slices” taken per second of the analog signal.</p>
</section>
<section id="analog-vs.-digital-signals" class="level3">
<h3 class="anchored" data-anchor-id="analog-vs.-digital-signals"><strong>Analog vs.&nbsp;Digital Signals</strong></h3>
<p>Analog signals continuously represent the signal’s magnitude, while digital signals present the magnitude only in fixed time intervals, resulting in a discrete representation. Analog signals appear as continuous graphs, whereas digital signals manifest as a step-like, stair-stepped graph.</p>
</section>
<section id="signal-analysis" class="level3">
<h3 class="anchored" data-anchor-id="signal-analysis"><strong>Signal Analysis</strong></h3>
<p>Signals are essentially sets of meaningful values arranged in a particular sequence. These values can be assessed from various perspectives, and this is where voice analytics comes into play.</p>
</section>
<section id="the-four-dimensional-framework-of-speech" class="level3">
<h3 class="anchored" data-anchor-id="the-four-dimensional-framework-of-speech"><strong>The Four-Dimensional Framework of Speech</strong></h3>
<p>Each and every signal can be completely described according to four distinct dimensions: (1) time, (2) amplitude, (3) frequency and (4) spectrum. These dimensions, when combined with listener perceptions, provide valuable insights into a speaker’s emotional state and personality traits. Understanding these dimensions enhances our grasp of human speech and the psychology behind it. Below we provide a concise overview of the different dimensions, readers interested in a more detailed overview, we recommend reading <a href="https://www.sciencedirect.com/science/article/pii/S0148296320306044">Voice analytics in business research: Conceptual foundations, acoustic feature extraction, and applications</a>.</p>
<section id="time-the-rhythm-of-speech" class="level4">
<h4 class="anchored" data-anchor-id="time-the-rhythm-of-speech"><strong>Time</strong>: The Rhythm of Speech</h4>
<p>The initial dimension, time, quantifies the duration of soundwaves, typically measured in seconds or milliseconds. This dimension encompasses various vocal attributes, such as speech and articulation rates (indicating speaking speed) and the percentage of unvoiced frames (reflecting pauses in speech). These features have been correlated with different personality traits and emotional states. Notably, a faster speech rate is often associated with a heightened perception of extraversion.</p>
</section>
<section id="amplitude-volume-control" class="level4">
<h4 class="anchored" data-anchor-id="amplitude-volume-control"><strong>Amplitude:</strong> Volume Control</h4>
<p>The second dimension assesses the intensity of the sound wave, expressed as power per unit area and commonly quantified in decibels (dB). Amplitude directly influences the perceived loudness of a sound, with higher values indicating greater volume. Perceived loudness is also associated with various personality traits and emotional states. For instance, voices with higher amplitudes, relative to a baseline, tend to be perceived as more dominant, assertive, and extroverted.</p>
</section>
<section id="frequency-the-musical-language-of-emotions" class="level4">
<h4 class="anchored" data-anchor-id="frequency-the-musical-language-of-emotions"><strong>Frequency:</strong> the Musical Language of Emotions</h4>
<p>The third dimension focuses on the frequency of a soundwave, typically measured in Hertz (Hz), representing the number of cycles per second of vibrating air particles. The human auditory range spans from 20 Hz to 20 KHz. Within the human voice, various frequencies are present, with the fundamental frequency (F0) being the most prominent, defining the perceived pitch (ranging from low to high). Like other dimensions, pitch is linked to multiple personality traits and emotional states. For instance, higher perceived pitch is associated with perceptions of greater competence, trustworthiness, or empathy. Notably, pitch strongly correlates with biological sex, as women typically exhibit higher pitches (ranging from 150 to 350 Hz) compared to men (ranging from 85 to 200 Hz).</p>
</section>
<section id="spectrum-the-art-of-vocal-patterns" class="level4">
<h4 class="anchored" data-anchor-id="spectrum-the-art-of-vocal-patterns"><strong>Spectrum:</strong> The Art of Vocal Patterns</h4>
<p>The fourth dimension delves into the perturbations within a soundwave. These perturbations are quantifiable through spectral features that gauge the degree of irregularity or periodicity in a soundwave, essentially measuring “vocal instability.” Noteworthy spectral features include shimmer (which quantifies variations in loudness), jitter (which assesses pitch fluctuations), and Harmonics-to-Noise Ratio (HNR, indicating additive noise in a signal). Once again, these measures correlate with various perceived personality traits and emotional states. For example, increased shimmer and jitter are linked to heightened stress or anger.</p>
</section>
</section>
<section id="a-four-dimensional-conceptual-framework-of-speech-linking-vocal-features-to-speaker-states-and-traits" class="level3">
<h3 class="anchored" data-anchor-id="a-four-dimensional-conceptual-framework-of-speech-linking-vocal-features-to-speaker-states-and-traits">A Four-Dimensional Conceptual Framework of Speech: Linking Vocal Features to Speaker States and Traits</h3>
<div class="table-wrapper">
<table class="table">
<thead>
<tr class="header">
<th>SOUNDWAVE DOMAIN</th>
<th>PRIMARY VOCAL FEATURES (EXAMPLE METRIC)</th>
<th>LISTENER PERCEPTION</th>
<th>INFERRED STATES AND TRAITS BASED ON EXPRESSED SPEECH</th>
<th>SELECTED RESEARCH</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Time</strong></td>
<td>Duration (Milli-/Seconds)</td>
<td>Duration of an Utterance</td>
<td>Anger↑, Fear↑, Sorrow↑</td>
<td><a href="https://doi.org/10.1121/1.1913238">Williams &amp; Stevens, 1972</a></td>
</tr>
<tr class="odd">
<td></td>
<td>Speech and Articulation Rate (Words per Second)</td>
<td>Velocity of Speech</td>
<td>Anger↑, Competence ↑, Contemplation↓, Dominance↓, Enthusiasm↑, Extraversion↑, Fear↑, Happiness↑, Persuasiveness↑, Sadness↓, Stress↑, Tenderness↓</td>
<td><a href="http://www.ncbi.nlm.nih.gov/pubmed/8117221">Brenner, Doherty, &amp; Shipp, 1994</a>; <a href="https://scholar.google.com/scholar?q=Dasgupta%2C%20P.%20B.%20(2017).%20Detection%20and%20analysis%20of%20human%20emotions%20through%20voice%20and%20speech%20pattern%20processing.%20International%20Journal%20of%20Computer%20Trends%20and%20Technology%2C%2052(1)%2C%201%E2%80%933.%2010.14445%2F22312803%2FIJCTT-V52P101.">Dasgupta, 2017</a>; <a href="https://doi.org/10.1037/0033-2909.129.5.770">Juslin &amp; Laukka, 2003</a>; <a href="https://scholar.google.com/scholar_lookup?title=Listener%20perception%20of%20time-compressed%20spokespersons&amp;publication_year=1982&amp;author=J.%20MacLachlan">MacLachlan, 1982</a>; <a href="https://doi.org/10.1037/0022-3514.34.4.615">Miller et al., 1976</a>; <a href="https://doi.org/10.1109/ACII.2015.7344614">Mohammadi &amp; Vinciarelli, 2015</a>; <a href="https://scholar.google.com/scholar_lookup?title=Social%20markers%20in%20speech&amp;publication_year=1979&amp;author=K.R.%20Scherer&amp;author=H.%20Giles">Scherer &amp; Giles, 1979</a>; <a href="https://doi.org/10.1093/hcr/26.1.148">Tusing &amp; Dillard, 2000;</a> <a href="https://doi.org/10.1121/1.1913238">Williams &amp; Stevens, 1972</a></td>
</tr>
<tr class="even">
<td></td>
<td>Voice breaks (Percentage Unvoiced Frames)</td>
<td>Number and Duration of Pauses</td>
<td>Competence↓, Contemplation↑, Extraversion↓</td>
<td><a href="https://scholar.google.com/scholar?q=Dasgupta%2C%20P.%20B.%20(2017).%20Detection%20and%20analysis%20of%20human%20emotions%20through%20voice%20and%20speech%20pattern%20processing.%20International%20Journal%20of%20Computer%20Trends%20and%20Technology%2C%2052(1)%2C%201%E2%80%933.%2010.14445%2F22312803%2FIJCTT-V52P101.">Dasgupta, 2017</a>; <a href="https://doi.org/10.1080/03637755809375240">Mallory &amp; Miller, 1958</a>; <a href="https://doi.org/10.1109/ACII.2015.7344614">Mohammadi &amp; Vinciarelli, 2015</a>; <a href="https://scholar.google.com/scholar_lookup?title=Social%20markers%20in%20speech&amp;publication_year=1979&amp;author=K.R.%20Scherer&amp;author=H.%20Giles">Scherer &amp; Giles, 1979</a></td>
</tr>
<tr class="odd">
<td><strong>Amplitude</strong></td>
<td>Intensity / Power (Sone)</td>
<td>Loudness of Speech</td>
<td>Aggression↑, Anger↑, Annoyance↑, Dominance↑, Extraversion↑, Fear↓, Happiness↑, Tenderness↓, Sadness↓, Shyness↓, Stress↑</td>
<td><a href="http://www.isca-speech.org/archive_open/speech_emotion/spem_110.html">Abelin &amp; Allwood, 2000</a>; <a href="http://www.ncbi.nlm.nih.gov/pubmed/8117221">Brenner et al., 1994</a>; J<a href="https://scholar.google.com/scholar_lookup?title=The%20effects%20of%20emotions%20on%20voice%20quality&amp;publication_year=1999&amp;author=T.%20Johnstone&amp;author=K.R.%20Scherer">ohnstone &amp; Scherer, 1999</a>; <a href="https://doi.org/10.1037/0033-2909.129.5.770">Juslin &amp; Laukka, 2003</a>; <a href="https://doi.org/10.1080/03637755809375240">Mallory &amp; Miller, 1958</a>; <a href="https://doi.org/10.1016/S0167-6393(02)00084-5">Scherer, 2003</a>; <a href="https://scholar.google.com/scholar_lookup?title=Social%20markers%20in%20speech&amp;publication_year=1979&amp;author=K.R.%20Scherer&amp;author=H.%20Giles">Scherer &amp; Giles, 1979</a>; <a href="https://doi.org/10.1093/hcr/26.1.148">Tusing &amp; Dillard, 2000</a></td>
</tr>
<tr class="even">
<td></td>
<td>Variability of Intensity / Power (Sone Variance)</td>
<td>Loudness Variability</td>
<td>Anger↑, Dominance↑, Fear↑, Happiness↑, Sadness↓, Tenderness↓</td>
<td><a href="https://doi.org/10.1037/0033-2909.129.5.770">Juslin &amp; Laukka, 2003</a>; <a href="https://doi.org/10.1093/hcr/26.1.148">Tusing &amp; Dillard, 2000</a></td>
</tr>
<tr class="odd">
<td><strong>Frequency</strong></td>
<td>Fundamental Frequency (Hertz)</td>
<td>Pitch</td>
<td>Anger↑, Competence↓, Confidence↓, Empathy↓, Extraversion↑, Fear↑, Happiness↑, Nervousness↑, Persuasiveness↓, Sadness↓, Stress↑, Tenderness↓, Trustworthiness↓</td>
<td><a href="https://doi.org/10.1037/0022-3514.37.5.715">Apple et al., 1979</a>; <a href="http://www.ncbi.nlm.nih.gov/pubmed/8117221">Brenner et al., 1994</a>; <a href="https://doi.org/10.1177/0146167218787805">Guyer et al., 2019</a>; <a href="https://doi.org/10.1037/0033-2909.129.5.770">Juslin &amp; Laukka, 2003</a>; <a href="https://doi.org/10.3758/s13423-016-1146-y">Oleszkiewicz et al., 2017</a>; <a href="https://scholar.google.com/scholar_lookup?title=Social%20markers%20in%20speech&amp;publication_year=1979&amp;author=K.R.%20Scherer&amp;author=H.%20Giles">Scherer &amp; Giles, 1979</a>; <a href="https://doi.org/10.1121/1.1913238">Williams &amp; Stevens, 1972</a></td>
</tr>
<tr class="even">
<td></td>
<td>Variability of Fundamental Frequency (Hertz Variance)</td>
<td>Pitch Variability</td>
<td>Anger↑, Extraversion↑, Happiness↑, Sadness↓, Shyness↓, Sociability↑, Tenderness↓</td>
<td><a href="http://www.isca-speech.org/archive_open/speech_emotion/spem_110.html">Abelin &amp; Allwood, 2000</a>; <a href="https://doi.org/10.1111/j.1468-2958.1990.tb00229.x">Burgoon, Birk, &amp; Pfau, 1990</a>; <a href="https://doi.org/10.1037/0033-2909.129.5.770">Juslin &amp; Laukka, 2003</a>; <a href="https://doi.org/10.1080/03637758609376141">Ray, 1986</a>; <a href="https://scholar.google.com/scholar_lookup?title=Social%20markers%20in%20speech&amp;publication_year=1979&amp;author=K.R.%20Scherer&amp;author=H.%20Giles">Scherer &amp; Giles, 1979</a></td>
</tr>
<tr class="odd">
<td><strong>Spectral</strong></td>
<td>Vocal Shimmer (cycle to cycle deviation from mean amplitude)</td>
<td>Loudness Perturbations</td>
<td>Anger↑, Confidence↑, Joy↓, Stress↑</td>
<td><a href="https://doi.org/10.1109/ICCSP.2016.7754275">Jacob, 2016; Jiang &amp; Pell, 2017</a>; <a href="https://doi.org/10.1109/ICASSP.2007.367261">Li et al., 2007</a></td>
</tr>
<tr class="even">
<td></td>
<td>Vocal Jitter (mean absolute difference between consecutive μs periods)</td>
<td>Pitch Perturbations</td>
<td>Anger↑, Annoyance↑, Happiness↑, Sadness↓, Stress↑</td>
<td><a href="https://scholar.google.com/scholar_lookup?title=The%20effects%20of%20emotions%20on%20voice%20quality&amp;publication_year=1999&amp;author=T.%20Johnstone&amp;author=K.R.%20Scherer">Johnstone &amp; Scherer, 1999</a>; <a href="https://doi.org/10.1037/0033-2909.129.5.770">Juslin &amp; Laukka, 2003</a>; <a href="https://doi.org/10.1109/ICASSP.2007.367261">Li et al., 2007</a></td>
</tr>
<tr class="odd">
<td></td>
<td>HNR (additive noise in signal in dB)</td>
<td>Voice Quality</td>
<td>Confidence↑, Happiness↑, Interest↑, Lust↓, Pleasure↑</td>
<td><a href="https://doi.org/10.1016/j.specom.2017.01.011">Jiang &amp; Pell, 2017</a>; <a href="https://doi.org/10.3758/s13423-019-01701-x">Kamiloglu ˘ et al., 2020</a></td>
</tr>
<tr class="even">
<td></td>
<td>Vocal Entropy (Shannon evenness of frequency spectrum)</td>
<td>Diversity of Vocal Transitions</td>
<td>Low mood↑</td>
<td><a href="https://doi.org/10.1109/SITIS.2016.113">Yingthawornsuk, 2016</a></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="r-for-voice-analytics" class="level2">
<h2 class="anchored" data-anchor-id="r-for-voice-analytics"><strong>R for Voice Analytics</strong></h2>
<p>At the Voice Analytics Hub, our tool of preference for voice analytics is R and, thus, all the tutorials and tools we provide focus on this language.</p>
<section id="why-r" class="level3">
<h3 class="anchored" data-anchor-id="why-r"><strong>Why R?</strong></h3>
<p>R is a versatile and powerful programming language and environment for statistical computing and data analysis, making it a valuable tool for sound analytics in several ways:</p>
<ol type="1">
<li><p><strong>Open Source:</strong> R is open-source software, which means it’s freely available to anyone. This accessibility encourages collaboration, allows for continual improvement, and reduces costs associated with proprietary software.</p></li>
<li><p><strong>Rich Ecosystem:</strong> R boasts a vast ecosystem of packages and libraries specifically designed for various data analysis tasks, including sound analytics. These packages provide functions and tools that streamline data processing, statistical analysis, and visualization, saving time and effort.</p></li>
<li><p><strong>Statistical Capabilities:</strong> R is renowned for its statistical capabilities, making it an ideal choice for analyzing sound data. Researchers can apply a wide range of statistical tests and models to explore patterns, trends, and relationships within sound datasets.</p></li>
<li><p><strong>Data Visualization:</strong> R excels at data visualization. It offers numerous packages (e.g., ggplot2) for creating publication-quality plots and visualizations, allowing researchers to effectively communicate their findings. Newspapers like The Economist or The Washington Post usually use R to create their nice looking plots.</p></li>
<li><p><strong>Flexibility:</strong> R’s flexibility enables users to adapt it to their specific needs. Researchers can write custom functions and scripts tailored to their sound analytics tasks, offering greater control and customization.</p></li>
<li><p><strong>Community Support:</strong> R has a large and active user community, which means that help is readily available through forums, mailing lists, and online resources. This support network can be invaluable when encountering challenges during sound analytics projects.</p></li>
<li><p><strong>Integration:</strong> R can seamlessly integrate with other programming languages and tools, facilitating the incorporation of sound analytics into broader data analysis workflows. For example, R can be used in conjunction with Python for machine learning tasks.</p></li>
<li><p><strong>Reproducibility:</strong> R’s script-based approach ensures that analyses are fully documented and reproducible. This is essential for maintaining transparency and rigor in sound analytics research.</p></li>
<li><p><strong>Cross-Platform Compatibility:</strong> R is available for multiple operating systems, including Windows, macOS, and Linux, ensuring compatibility with various computing environments.</p></li>
<li><p><strong>Community Contributions:</strong> The R community actively develops and contributes new packages and functionalities. This means that the toolset for sound analytics in R is continually evolving and expanding.</p></li>
</ol>
<p>In summary, R’s open-source nature, statistical prowess, data visualization capabilities, flexibility, and strong community support make it a compelling choice for sound analytics. Researchers can leverage R’s extensive ecosystem to effectively analyze, visualize, and draw insights from sound data for a wide range of applications, including speech analysis, audio processing, and more.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>