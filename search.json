[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Voice Analytics Hub",
    "section": "",
    "text": "In an age defined by rapid technological advancements, the integration of natural language into voice-controlled interfaces is reshaping the way humans interact with technology. Voice-controlled interfaces, exemplified by Amazon Alexa, Google Assistant, and Siri, signify a paradigm shift in human-machine interaction – not mere tools, but the cornerstone of a new era.\nImpressive Presence, Astonishing Growth\nWith over 100 million smart speakers globally and projected sales reaching $31.82 billion by 2025, it’s clear that voice-controlled interfaces are poised to redefine how we search, shop, and express preferences."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About The Voice Analytics Hub",
    "section": "",
    "text": "Unleashing the Power of Voice\nIn today’s digital age, voice data has emerged as a powerful source of insight. Human voices carry a wealth of information, from emotions and personality traits to consumer preferences and behavioral patterns. At The Voice Analytics Hub, we believe in harnessing this untapped potential to advance research and understanding across various domains.\nA Holistic Approach\nOur platform goes beyond mere analysis; it’s a holistic approach to voice analytics. We understand that every researcher is unique, and their needs vary. That’s why we offer a wide range of tools, resources, and support to cater to diverse research objectives. Whether you’re delving into consumer sentiment analysis, exploring the psychology of voice, or conducting in-depth behavioral studies, we’ve got you covered.\nEmpowering Researchers\nThe Voice Analytics Hub is more than just a set of tools; it’s a community of passionate researchers dedicated to pushing the boundaries of what’s possible with voice data. We empower researchers to:\n\nExplore Voice Data: Dive into the rich world of voice data and extract valuable insights that drive your research forward.\nAccess Cutting-Edge Tools: Benefit from state-of-the-art voice analytics tools and resources designed to streamline your work.\nCollaborate and Learn: Connect with like-minded researchers, share knowledge, and stay at the forefront of voice analytics advancements.\nStay Informed: Stay updated with the latest trends, best practices, and research findings in the field of voice analytics.\n\nWhy Voice Analytics?\nVoice analytics is not just a trend; it’s a transformative force in research. By tapping into the nuances of human voice, researchers can uncover hidden truths, predict consumer behavior, and gain a deeper understanding of human psychology. Whether you’re a seasoned expert or new to the world of voice analytics, The Voice Analytics Hub is your ideal partner on this exciting journey.\nJoin Our Community\nWe invite you to be a part of our growing community of researchers and explorers. Whether you’re an academic, a market researcher, a psychologist, or anyone intrigued by the potential of voice data, you’ll find a welcoming space here. Let’s embark on this voice analytics adventure together.\nGet Started Today\nReady to unlock the potential of voice analytics for your research? Join The Voice Analytics Hub and discover a world of possibilities. Together, we’ll unravel the secrets hidden within the human voice and pave the way for groundbreaking discoveries.\nWelcome to The Voice Analytics Hub—where the science of voice meets the art of exploration!"
  },
  {
    "objectID": "index.html#unveiling-the-power-of-the-human-voice",
    "href": "index.html#unveiling-the-power-of-the-human-voice",
    "title": "Welcome to the Voice Analytics Hub",
    "section": "Unveiling the Power of the Human Voice",
    "text": "Unveiling the Power of the Human Voice\nHowever, unlocking the full potential of voice-enabled technology requires a profound understanding of speech and psychology. It demands the ability to decipher and analyze vocal features within human voice data, encompassing dimensions like pitch, loudness, and speech pauses. In this auditory symphony, valuable insights await discovery.\nA Mirror to Emotions\nConsider this: when a user grows frustrated with a voice assistant’s errors, their voice may mirror their exasperation. Amidst tense traffic, anxiety surfaces through accelerated speech. Conversely, a tired employee’s voice may convey monotony as they request Mozart’s Requiem. The human voice faithfully reflects emotional states."
  },
  {
    "objectID": "index.html#beyond-digital-interfaces-the-universal-language",
    "href": "index.html#beyond-digital-interfaces-the-universal-language",
    "title": "Welcome to the Voice Analytics Hub",
    "section": "Beyond Digital Interfaces: The Universal Language",
    "text": "Beyond Digital Interfaces: The Universal Language\nThe significance of the human voice transcends voice-controlled interfaces. It permeates personal, professional, and public interactions. Our voices reveal thoughts, emotions, and intentions. From everyday conversations to persuasive speeches, our voices are powerful tools of expression.\nUnearthing Deeper Insights\nThe human voice is a dynamic instrument that extends beyond technology. It connects, expresses, and communicates. Voice analytics, therefore, holds immense potential not only for enhancing digital experiences but also for unraveling the intricate tapestry of human communication. It empowers us to delve into the layers of meaning embedded in our voices, fostering deeper insights, empathy, and more effective interactions."
  },
  {
    "objectID": "index.html#charting-a-new-path",
    "href": "index.html#charting-a-new-path",
    "title": "Welcome to the Voice Analytics Hub",
    "section": "Charting a New Path",
    "text": "Charting a New Path\nWhile research in speech and voice exists, investigations into the transformative impact of voice technology on users are limited. Our mission sets us apart. At the Voice Analytics Hub, we’re dedicated to providing individuals with the foundational knowledge and resources to embark on a transformative journey into the world of voice analytics.\nYour Gateway to Voice Analytics\nWhether you’re a curious novice or a seasoned professional, our platform equips you with the essential tools to explore this dynamic field. Join us on this exciting journey, where we empower you to navigate the uncharted territory of voice analytics."
  },
  {
    "objectID": "basic-tutorial.html",
    "href": "basic-tutorial.html",
    "title": "The voice Analytics Hub",
    "section": "",
    "text": "For our comprehensive analysis, we began by extracting the publicly available audio file from YouTube and converting it into the Waveform audio format. Our investigation is centered on two pivotal aspects of this interaction:\n\nSpeech Formation of the Wakeword “Alexa”\nVocal Changes During the Issuance of a Command (“Alexa, play something is cooking in my kitchen on Spotify by Dana”)\n\nTo facilitate our analysis, we meticulously edited the voice recordings, retaining only the segments containing the three consecutive wakewords and the two subsequent user commands. It is noteworthy that the third utterance of “Alexa” was not associated with a command but rather followed by a barrage of profanity and abusive language. Consequently, our subsequent sections will delve into the examination of this specific case.\nOur analytical approach primarily leverages the renowned seewave package, which has emerged as the gold standard in R-sound analysis. This versatile package encompasses an impressive array of 130 functions designed for the analysis, manipulation, representation, editing, and synthesis of time-based audio waveforms. While seewave serves as our cornerstone, we also make reference to other valuable packages, such as tuneR, soundgen, and phonTools, for their specialized functionalities as needed."
  },
  {
    "objectID": "basic-tutorial.html#data-acquisition-and-processing",
    "href": "basic-tutorial.html#data-acquisition-and-processing",
    "title": "The voice Analytics Hub",
    "section": "Data Acquisition and Processing",
    "text": "Data Acquisition and Processing\n\nReading Sound Files\nAs previously mentioned, the primary focus of this tutorial centers around the utilization of the seewave package. While it is important to note that seewave lacks native capabilities for sound file reading, we adeptly overcome this limitation by harnessing functions from complementary packages. It is crucial to emphasize that different packages generate distinct classes of sound objects, each optimized for specific sound manipulation tasks. Consequently, when choosing an alternative package to load sound data, it becomes paramount to consider this inherent class compatibility.\nIn the context of seewave, its core functionality is primarily tailored to work seamlessly with sound objects of the Wave class. These Wave class sound objects are conventionally created using the tuneR package. Hence, when working with seewave, it is strongly recommended to employ tuneR for sound data loading.\nThe process of loading the two user commands from the interaction with Amazon Alexa and saving these sound files as new objects, cmd1 and cmd2, is accomplished as follows:\nTo begin, we employ the readWave() function from the tuneR package. This function efficiently loads or reads a sound file from a specified location. Here is the step-by-step procedure for loading and saving the user’s interaction commands:\n\nlibrary(tuneR)\ncmd1 <- readWave(\"alexa_cmd1.wav\")\ncmd2 <- readWave(\"alexa_cmd2.wav\")\n\nUpon invoking these newly created objects, we obtain an informative output that reveals the underlying structure of the objects. This output not only underscores the seamless integration of the ‘wave’ class objects generated by the tuneR package but also provides key insights into their fundamental characteristics. These characteristics encompass:\n\nNumber of Samples: This indicates the total count of discrete data points in the audio waveform.\nDuration (in seconds): The elapsed time in seconds, capturing the length of the audio.\nSampling Rate (in Hertz): Denoting the rate at which individual samples are taken per second.\nNumber of Channels: It signifies whether the audio is mono (single channel) or stereo (two channels).\nBit Rate: Representing the number of bits processed per unit of time.\n\nUpon inspecting both objects, it becomes evident that they share identical sampling rates, channel numbers, and bit rates. However, a notable distinction emerges in their duration, with the second sound file being 820 milliseconds longer than the first. This divergence in duration warrants further investigation and could potentially yield valuable insights into the audio data.\n\ncmd1\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\ncmd2\n\n\nWave Object\n    Number of Samples:      137881\n    Duration (seconds):     3.13\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nFurthermore, the readWave() function offers additional parameters, notably from and to, which facilitate the selection of specific segments within the audio data for reading. By default, these parameters operate in samples units, defining the segment based on sample counts. However, the readWave() function also introduces the units argument, affording the flexibility to modify the units of from and to to seconds, minutes, or hours.\nTo illustrate, suppose we wish to load only the initial 0.5 seconds and the segment from 0.5 seconds to 2 seconds from the audio file and store them in separate objects, denoted as cmd1.s1 and cmd1.s2, respectively. Achieving this precision is straightforward, involving the configuration of the from, to, and units arguments as demonstrated below:\n\n(cmd1.s1 <- readWave(\"alexa_cmd1.wav\",from=0,to=0.5,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      22050\n    Duration (seconds):     0.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(cmd1.s2 <- readWave(\"alexa_cmd1.wav\",from=0.5,to=2,units=\"seconds\"))\n\n\nWave Object\n    Number of Samples:      66150\n    Duration (seconds):     1.5\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\nPlaying sound file\nSound analysis is an iterative process, requiring frequent listening to parts of a soundwave. Although, R itself cannot play sound files the seewave’s listen( ) function allows us to call the default audio player of the user’s operating system. Thus, we could play the previously loaded cmd1 variable by using the listen() function:\n\nlibrary(seewave)\nlisten(cmd1)\n\nMuch like the readWave() function, the listen() function offers analogous from and to arguments to precisely determine the sections of interest for auditory playback. Furthermore, it introduces an f argument, providing the capability to manipulate the sampling frequency rate. This adjustment allows for the alteration of the original sound’s pitch, facilitating the creation of both higher and lower-pitched auditory renditions. The code snippet below demonstrates this transformative process:\n\nlisten(cmd1, f=cmd1@samp.rate*1.1)\n\n\nlisten(cmd1, f=cmd1@samp.rate/1.1)\n\n\n\nEditing sound Files\nIn certain applications, it becomes necessary to perform additional edits on voice files. These edits could include tasks such as (1) extracting specific segments of a soundwave for in-depth analysis, (2) eliminating a series of utterances from a soundwave, (3) trimming periods of silence at the beginning or end of a sound file, and (4) filtering out all unvoiced frames from a sound file.\nBoth the tuneR and seewave packages offer a suite of functions designed to address these various editing procedures:\n\nextractWave( ): This function facilitates the extraction of desired segments from a soundwave. Users can specify the segments using the from and to arguments, which we discussed earlier. The extractWave() function defaults to samples as the unit, but this can be adjusted using the ‘xunit’ argument.\ndeletew( ): To remove specific portions from a soundwave, the deletew() function is employed. By default, it operates in units of time.\nnoSilence( ): This function is particularly useful for removing periods of silence from the beginning or end of a sound file. Users can fine-tune this process by specifying the desired silence level.\nzapsilw( ): To eliminate all unvoiced frames from a sound file, the zapsilw() function comes into play. Users can tailor this operation by setting the ‘threshold’ argument, which measures the amplitude threshold in percent distinguishing silence from signal. Additionally, the zapsilw() function offers the default feature of generating oscillograms for both the original sound file and the modified version post unvoiced frame removal, providing visual insight into the process.\n\nThese functions empower users to efficiently manipulate sound files, ensuring they are tailored to meet the specific requirements of their analyses. To illustrate their practical utility, let’s delve into some illustrative examples.\nAs an instance, consider the application of the extractWave() function to pinpoint the initial 700 milliseconds of the soundwave residing in cmd1.\n\n#Extract first 700ms\ncmd1.xtr <- extractWave(cmd1, from = 0, to = 0.7, xunit = \"time\") \n\nHere, the from and to arguments are employed to specify the time range of interest. In this case, we isolate the first 700 milliseconds, facilitating a focused analysis.\nAlternatively, instead of extracting this segment, we can achieve its removal using the deletew() function:\n\n#Delete first 700ms\ncmd1.rem <- deletew(cmd1, from=0, to=0.7, output=\"Wave\") \n\nContinuing with our exploration of sound manipulation, we can also replicate the first 700 milliseconds threefold with the repw() function:\n\n#Repeat first 700ms three times\ncmd1.rep3 <- repw(cmd1, f=cmd1@samp.rate, times=3, output=\"Wave\")\n\nTo refine the audio data further, we may opt to remove solely the unvoiced segments at the beginning and end using the noSilence() function:\n\n#Remove only unvoiced start and ending\ncmd1.cut <- noSilence(cmd1)\ncmd1.cut\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\ncmd1\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nIn this particular instance, a comparison between the original sound cmd1 and the processed version cmd1.cut, where we have removed unvoiced segments from the beginning and end, reveals no discernible alteration. This lack of change stems from the fact that cmd1 initially did not contain any unvoiced segments at either its outset or conclusion.\nAlternatively, we could use the zapsilw() function to cleanse all the unvoiced elements of cmd1:\n\n#Remove all unvoiced frames of a soundwave\ncmd1.nosil <- zapsilw(cmd1, threshold=1, output=\"Wave\")\ncmd1\n\n\nWave Object\n    Number of Samples:      102051\n    Duration (seconds):     2.31\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\ncmd1.nosil\n\n\nWave Object\n    Number of Samples:      62425\n    Duration (seconds):     1.42\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\n\n\n\nFigure 1: Oscillograms of First and Original Command (Upper Panel) and with voice breaks removed (Lower Panel)\n\n\n\n\nIn this case, a close examination of Figure 1 reveals that the zapsilw() function has been remarkably successful in eliminating all the unvoiced areas within the soundwave. This process has resulted in a cleaner and more refined audio representation.\nFurthermore, when we compare cmd1 to cmd1.nosil, it becomes evident that the duration of the latter is noticeably shorter, clocking in at almost 1 second less. This reduction in duration underscores the effective removal of unvoiced segments, affirming the utility of this process in streamlining the audio data while preserving its crucial vocal elements.\n\n\nWriting sound files\nFollowing sound file editing, it is often essential to preserve the revised version for future use. To accomplish this, the seewave package offers the convenient savewav( ) function, designed explicitly for storing R sound objects as .wav files. The process involves specifying the following parameters:\n\nR Sound Object: As the first argument, designate the R sound object that you intend to save as a .wav file.\nSampling Frequency (f): Next, specify the sampling frequency to be associated with the saved .wav file.\nFilename (filename): Finally, provide the desired filename under which the edited sound object will be stored.\n\nIt’s worth noting that if the sampling frequency is not explicitly defined, the seewave package will automatically utilize the same sampling frequency as the edited R object, simplifying the saving process.\nAs an illustrative step, let’s go ahead and save the modified version of cmd1, where all unvoiced segments have been successfully removed, as a .wav file within our system:\n\nsavewav(cmd1.nosil, filename = \"cmd1_noSilence.wav\")"
  },
  {
    "objectID": "basic-tutorial.html#visualizing-sound",
    "href": "basic-tutorial.html#visualizing-sound",
    "title": "The voice Analytics Hub",
    "section": "Visualizing sound",
    "text": "Visualizing sound\nHaving covered the processes of reading, editing, and saving sound objects, we now embark on the journey of visualizing the essential attributes of a sound wave. Visualization entails the transformation of a sound wave into a graphical or statistical representation. The primary means of depicting a sound wave typically involve showcasing its (1) amplitude, (2) frequency, and (3) a blend of amplitude and frequency variations over time. Two common visualizations employed for this purpose are oscillograms, which capture amplitude, and spectrograms, which provide insights into frequency and the interplay between frequency and amplitude over time.\n\n\nOscillograms offer a visual representation of the instantaneous amplitude of a soundwave plotted against time. They are often referred to as waveforms, as they graphically depict the variations within the sound wave itself. Oscillograms serve as valuable tools for discerning potential changes in loudness over time within a soundwave. In R, you can create oscillograms using the oscillo() function from the seewave package. This function requires just one argument, the sound object. Moreover, oscillo() provides the flexibility to customize various visual aspects, such as the title (using the title argument), label color (via the collab argument), and wave color (by setting the ‘colwave’ argument). Additionally, you can specify the ‘from’ and ‘to’ arguments, similar to what we did during data processing, to generate an oscillogram for a specific time interval in seconds.\nTo gain insights from the oscillograms of the two Alexa commands, we aim to first visualize the entire soundwave and then zoom in to focus solely on the articulation of the wakeword.\nTo plot all four graphs within a single plotting region, we partition the plot into four distinct sections using the standard par and mfrow arguments in R. Furthermore, to exclusively display the wakeword, we make use of the from and to arguments within the oscillo() function.\n\n\n\n\n\nFigure 2: Oscillograms of First and Second Command (Upper Panel) and Wakeword (Lower Panel)\n\n\n\n\nUpon closer examination, as depicted in Figure 2, a few notable observations come to light at first glance:\n\nDifference in Duration: It becomes apparent that the second command is slightly longer than the first. Remarkably, both commands share identical content, both stating “Alexa, play by Dana.” A deeper dive into the oscillograms reveals the underlying reason: in the second command, there are longer vocal breaks, representing the time gaps between each spoken word.\nEmphasis on Individual Words: Notably, there are striking variations in the emphasis placed on individual words when issuing the two commands. For instance, in the lower panel of Figure 2, we can discern the pronounced differences in overall amplitudes. Particularly, the word “Alexa” in the second command exhibits significantly greater amplitude compared to its counterpart in the first command. This observation suggests that, without needing to listen to the voice files or comprehend the spoken content, we can already deduce that the second command tends to be louder overall than the first. Such a distinction may hint at potential heightened emotions such as anger, stress, or irritability on the part of the speaker.\n\nThese initial insights gleaned from the oscillograms provide valuable cues for further analysis and interpretation of the audio data.\n\n\nVisualizing Fundamental Frequency\nThe graphical representation of the fundamental frequency over time, often referred to as an f0 contour or pitch track, holds significant value in acoustic analysis. This visualization unveils essential aspects, including the speaker’s fundamental frequency range, pitch variations throughout speech, distinctions between voiceless and voiced segments, as well as patterns of regular and irregular phonation.\nIn the context of the seewave package, the fund() function takes center stage. It skillfully estimates the fundamental frequency of an R object, employing a short-term cepstral transform, and automatically generates a visual plot of the fundamental frequency.\nTo demonstrate the practical application and insights derived from this analysis, we narrow our focus to the wakewords uttered by the speaker during their interaction with Amazon Alexa. It’s worth noting that although the user issued only two complete commands, the interaction involved three distinct wakewords. The third wakeword, notably, did not lead to the issuance of a command but instead featured the use of offensive language directed at Alexa.\nTo proceed with this exploration, we initiate the process by reading these distinct wakewords using the readWave() function:\n\n(w1 <- readWave(\"alexa_wakeword_1.wav\"))\n\n\nWave Object\n    Number of Samples:      30902\n    Duration (seconds):     0.7\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(w2 <- readWave(\"alexa_wakeword_2.wav\"))\n\n\nWave Object\n    Number of Samples:      32804\n    Duration (seconds):     0.74\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n(w3 <- readWave(\"alexa_wakeword_3.wav\"))\n\n\nWave Object\n    Number of Samples:      46116\n    Duration (seconds):     1.05\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\n\nUpon inspecting the output generated after calling each sound object, we readily observe differences in the durations of these wakewords. Specifically, they span 0.7 seconds, 0.74 seconds, and 1.05 seconds, respectively, highlighting the temporal distinctions among them.\nWith the three wakewords now successfully imported into R, we proceed to concatenate them into a single soundwave using seewave’s bind() function. We undertake this concatenation to facilitate a comprehensive visual evaluation of how the fundamental frequency evolves from one command to another.\n\n(wake_all <- bind(w1,w2,w3))\n\n\nWave Object\n    Number of Samples:      109822\n    Duration (seconds):     2.49\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Stereo\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \n\nlisten(wake_all)\n\nThis amalgamation sets the stage for the application of the fund() function to the concatenated sound object. Additionally, we leverage the ‘threshold’ argument within the fund() function, allowing us to eliminate smaller amplitude variations during signal detection, specified as a percentage. The results generated by the fund() function, represented as a matrix with two columns (x denoting time and y representing fundamental frequency), are stored in a new variable ff. Subsequently, to illustrate the evolution of the speaker’s fundamental frequency over time, we employ the time and frequency parameters in a linear model. This modeling unveils an upward trend, as portrayed in Figure 3, showcasing the progression from the first to the second and ultimately the third utterance of the wakeword “Alexa.”\n\nff <- fund(wake_all,threshold=1)\nmod <- lm(ff[,2]~ff[,1])\nabline(mod,col=\"red\",lwd=2,lty=2)\n\n\n\n\nFigure 3: Increasing Fundamental Over Time from First, Second, to Third Wakeword\n\n\n\n\nTo enhance our analysis further, we employ the formanttrack() function from the phonTools package. This function serves as a practical tool to track the distribution of acoustic energy across various frequency bands, typically in 1000Hz intervals. It’s worth noting that the phonTools package exclusively supports mono audio. Therefore, when working with stereo files, as in this scenario, only one of its channels must be specified as an argument. The channel of a wave object can be accessed using the @ operator along with the name of the desired channel (left or right). Additionally, since phonTools is not specifically designed to handle wave objects, the sampling frequency of the sound file must be manually set using the fs argument.\nFigure 4 illustrates the first three frequency bands for each wakeword, respectively. These displays complement our earlier observations, illustrating the frequency escalation from the first to the third wakeword, along with an overall increase in variability over time. Given the strong correlation between heightened frequency levels and experiences of stress, anger, or frustration, these visualizations further underscore the extended duration during which the user encountered difficulties in their interaction with Amazon Alexa.\n\nlibrary(phonTools)\npar(mfrow=c(1,3))\nformanttrack(w1@left, fs=w1@samp.rate, formants=3, periodicity=.5)\nformanttrack(w2@left, fs=w2@samp.rate, formants=3, periodicity=.5)\nformanttrack(w3@left, fs=w3@samp.rate, formants=3, periodicity=.5)\n\n\n\n\nFigure 4: Visualizing the First (Black), Second (Red), and Third (Green) Formants Across Wakewords\n\n\n\n\n\n\nSpectrograms\nSpectrograms provide a rich, multi-dimensional portrayal of a soundwave, offering insights into its composition. In this representation, time unfolds along the x-axis, frequency extends along the y-axis, and a third dimension portrays amplitude levels (loudness) through varying color codes. The seewave package equips us with the spectro() function, which constructs a spectrographic visualization of a time wave. This versatile function demands only a time wave object, such as a sound object, as its input.Furthermore, the spectro() function offers several customization options to tailor the appearance of the spectrogram. For instance, the flim argument permits us to specify the minimum and maximum frequencies displayed. Additionally, osc introduces an oscillogram at the bottom of the spectrogram plot, while dBref allows us to define a reference value for the dB range of amplitude.\nFigure 5 provides a more nuanced understanding, summarizing the significant distinctions between the initial two commands and the subsequent three consecutive wakewords spoken by the user. Across all spectrograms, three noteworthy points emerge:\n\nIncrease in Loudness: There is a noticeable escalation in power or loudness, depicted by the deepening reddish color gradients.\nProlonged Voice Breaks: A trend of longer voice breaks becomes evident from the first command to the second.\nFrequency Stability: The first command exhibits relatively stable frequency, whereas the second command displays a moderate upward trend.\n\nCollectively, these observations suggest that the heightened frequency, amplified loudness, and extended voice breaks likely mirror the speaker’s experiences of tension, anger, stress, and frustration during their interaction with Amazon Alexa.\n\nspectro(cmd1, osc=TRUE, flim=c(0,6), dB = \"max0\", dBref = 2*10e-5)\nspectro(cmd2, osc=TRUE, flim=c(0,6), dB = \"max0\", dBref = 2*10e-5)\nspectro(wake_all, osc=TRUE, flim=c(0,6), dB = \"max0\", dBref = 2*10e-5) \n\n\n\n\n\n\n\n\n\nFigure 5: Spectrogram of the User’s Commands and First Three Wakewords"
  },
  {
    "objectID": "basic-tutorial.html#acoustic-feature-extraction",
    "href": "basic-tutorial.html#acoustic-feature-extraction",
    "title": "The voice Analytics Hub",
    "section": "Acoustic Feature Extraction",
    "text": "Acoustic Feature Extraction\nIn addition to the tasks we’ve explored thus far, the extraction of acoustic characteristics from sound files holds paramount importance in their analysis. These extracted features serve a variety of purposes, whether utilized as predictors or outcomes in statistical models. In this section, we shed light on crucial functions spread across different packages for extracting these vocal attributes, categorizing them into distinct domains: time, amplitude, frequency, and spectral.\n\nTime Domain\nThe most fundamental measure in the time domain is the duration, typically expressed in seconds or milliseconds, which quantifies the temporal extent of a soundwave. The duration( ) within the seewave package offers a straightforward means of extracting this duration, providing the duration of the sound object in seconds. When we apply this function to the two commands, we ascertain that the first command has a duration of 2.31 seconds, while the second command spans 3.13 seconds.\n\nduration(cmd1)\n\n[1] 2.314082\n\nduration(cmd2)\n\n[1] 3.126553\n\n\nSimilarly, when we apply the same procedure to the wakewords, we discover that they have durations of 0.7 seconds, 0.74 seconds and 1.05 seconds, respectively.\n\nduration(w1)\n\n[1] 0.7007256\n\nduration(w2)\n\n[1] 0.7438549\n\nduration(w3)\n\n[1] 1.045714\n\n\nThe soundgen package includes the analyze() function, which provides the ability to extract several features across the time, amplitude, frequency, and spectral domain respectively. For example, using this function we can directly extract the number of voiced and unvoiced frames of a sound object. To do so, the results from the soundgen function should be stored in an R object. This object results in a list of two data.frames: a detailed data.frame (you can retrieve it by using $detailed) in which each row represents a Short-Time Fourier Transform frame and each column represents a vocal feature and a summarized data.frame (you can retrieve it by using $summary).\nThus, we can proceed to use the analyze() function to extract several vocal features from the first and second commands:\n\nlibrary(soundgen)\nfeat_cmd1 <- analyze(\"alexa_cmd1.wav\", plot = F)\nfeat_cmd2 <- analyze(\"alexa_cmd2.wav\", plot = F)\n\nSince the summary object represents a data.frame, we can access its columns using the $ operator followed by the column name. Hence, in order to check which proportion of frames are voiced, we can directly call the voiced column from the output extracted by the analyze() function:\n\n#Returns the proportion of voiced samples\nfeat_cmd1$summary$voiced\n\n[1] 0.4395604\n\nfeat_cmd2$summary$voiced\n\n[1] 0.3145161\n\n\nRevealing a a greater percentage of vocal breaks in the second (43.96%) compared to the first (31.45%) command.\n\n\nAmplitude Domain\nThe amplitude of a soundwave dictates its power or loudness, with smaller amplitudes indicating softer sounds and larger amplitudes indicating louder ones. It essentially measures how far air particles deviate from their equilibrium position. To calculate the amplitude at various points in time, we can utilize the oscillo function from the seewave package, as demonstrated earlier, with its plot argument set to FALSE. This provides us with various statistics, such as the maximum and minimum amplitudes. In the code snippet below, we observe that the first command generally had a higher volume compared to the second, evident in the smaller minimum and larger maximum values.\n\nmin(oscillo(cmd1, plot = F))\n\n[1] -28068\n\nmax(oscillo(cmd1, plot = F))\n\n[1] 19895\n\nmin(oscillo(cmd2, plot = F))\n\n[1] -29746\n\nmax(oscillo(cmd2, plot = F))\n\n[1] 20029\n\n\nAnother method to estimate soundwave amplitude involves computing the root-mean-squared (RMS) of the amplitude envelope. This can be accomplished by combining the rms() function to calculate the RMS and the env() function to calculate the envelope. This approach provides us with insight into the sound file’s average loudness, revealing that, on average (after removing silent portions), the first command was louder than the second.\nThe Root Mean Square (RMS) is calculated as follows:\n\\[RMS = \\sqrt{\\frac{1}{N}\\sum_{n=1}^{N} x_i^2}\\] Here, \\(x_i\\) represents each amplitude envelope point and \\(N\\) represents the total number of points.\n\npar(mfrow=c(1,2))\nrms(env(zapsilw(cmd1, plot = F),f=cmd1@samp.rate))\n\n[1] 8771.827\n\nrms(env(zapsilw(cmd2, plot = F),f=cmd1@samp.rate))\n\n[1] 9386.833\n\n\n\n\n\nFigure 6: Amplitude envelope of Command 1 (right) and Command 2 (left)\n\n\n\n\nMoreover, the analyze() function from the soundgen package yields a subjective unit of loudness measured in sone. This measure is stored in the ‘loudness’ column within the data.frame produced by analyze(). Using the mean() function in R on the loudness column computes the average loudness in sone, reaffirming our earlier observations that the second command was generally louder than the first.\n\nfeat_cmd1$summary$loudness_mean\n\n[1] 15.72149\n\nfeat_cmd2$summary$loudness_mean\n\n[1] 17.66282\n\n\nAdditionally, soundgen provides the getLoudness() function, offering a visual representation of loudness spectrum for soundwaves. It presents a grayscale visualization of the spectrum, with darker areas indicating higher sound pressure levels, and thus, higher loudness.\n\nloudness1 <- getLoudness(\"alexa_cmd1.wav\", plot = FALSE)\n\nloudness2 <- getLoudness(\"alexa_cmd2.wav\", plot = FALSE)\n\n\n\n\n\n\nFrequency domain\nIn this section we focus on the extraction of two main features in the frequency domain: (1) pitch of soundwave and (2) the extent of variability in frequency of the sound-wave. Both of them can be directly extracted from the summary of the results extracted from soundgen’s analyze( ) function: (1) pitch_mean and (2) pitch_sd. Consistent with our expectation, we find a strong increase from 219.76Hz to 245.76Hz in the average pitch. Thus, the extracted features confirm the “shrill” and aroused sound of the user’s voice after the repeated failure from the first to the second voice command.\n\nfeat_cmd1$summary$pitch_mean\n\n[1] 219.7559\n\nfeat_cmd2$summary$pitch_mean\n\n[1] 245.7643\n\nfeat_cmd1$summary$pitch_sd\n\n[1] 62.30056\n\nfeat_cmd2$summary$pitch_sd\n\n[1] 32.96\n\n\nTo get into a higher level of granularity, we can apply those same analysis only to the wakeword “Alexa”. In that way, we can see how these key differences are even more prominent: 173.68Hz on the first wakeword, 278.75Hz on the second and 296.46Hz on the third.\n\nfeat_w1 <- analyze(\"alexa_wakeword_1.wav\", plot = F)\nfeat_w2 <- analyze(\"alexa_wakeword_2.wav\", plot = F)\nfeat_w3 <- analyze(\"alexa_wakeword_3.wav\", plot = F)\n\nfeat_w1$summary$pitch_mean\n\n[1] 173.677\n\nfeat_w2$summary$pitch_mean\n\n[1] 278.7515\n\nfeat_w3$summary$pitch_mean\n\n[1] 296.4579\n\nfeat_w1$summary$pitch_sd\n\n[1] 14.07013\n\nfeat_w2$summary$pitch_sd\n\n[1] 103.6117\n\nfeat_w3$summary$pitch_sd\n\n[1] 110.608\n\n\n\n\nSpectral domain\nSpectral features of a soundwave reflect perturbances of a soundwave. Measures of spectral qualities of a soundave generally assess the amount of perturbance or periodicity of sound. Two such measure of perturbances can be directly extracted using the Harmonics-to-Noise ratio (HNR) and level of entropy of the soundwave from the analyze( ) function of the soundgen package. Comparing the level of entropy and periodicity between the two commands confirms the moderately larger level of entropy and greater perturbances in the second compared to the first command respectively.\n\nfeat_cmd1$summary$entropy_mean\n\n[1] 0.1250395\n\nfeat_cmd2$summary$entropy_mean\n\n[1] 0.1437937\n\n\n\nfeat_cmd1$summary$HNR_mean\n\n[1] 5.90924\n\nfeat_cmd2$summary$HNR_mean\n\n[1] 5.422622"
  },
  {
    "objectID": "Basic Theory.html",
    "href": "Basic Theory.html",
    "title": "The voice Analytics Hub",
    "section": "",
    "text": "Voice analytics is a multidisciplinary exploration of human speech, seamlessly integrating phonetics, acoustics, and cognitive psychology. This cutting-edge field unravels the intricacies of speech formation, transmission, and perception, offering profound insights into human communication.\nBefore diving into the world of voice and audio analytics, it’s essential to grasp the fundamental principles of acoustics. While acoustics is a vast field, we’ll cover some key concepts to provide a foundational understanding of this fascinating domain. These insights will not only enhance your comprehension of voice analytics but also empower you to better interpret your analysis results.\n\n\n\n\nSound, at its core, is a wave in motion—a concept that traces its roots to the study of water waves. These sound waves travel through various elastic media, including air, steel, and water, inducing changes in pressure and motion. Elastic media have the remarkable ability to reshape themselves when subjected to external forces.\n\n\n\nWaves can be categorized based on the direction of their movement, resulting in three primary types: transverse, longitudinal, and surface waves.\n\nTransverse Waves: In transverse waves, the particles in the medium move in a direction perpendicular to the wave’s propagation direction.\nLongitudinal Waves: Longitudinal waves involve particles moving parallel to the direction of the wave’s travel.\nSurface Waves: These waves exhibit circular particle motion, typically occurring at the interface between two media.\n\nIn the ideal fluid medium, such as air and other gases, sound is transmitted as longitudinal waves. However, in solid mediums like steel, sound can travel as both longitudinal and transverse waves.\n\n\n\nVoice and musical instruments produce complex periodic sounds, characterized by two key attributes: periodicity and complexity. Periodicity refers to the regular repetition of sound patterns, while complexity indicates the presence of multiple signal components, known as harmonic frequencies. In voice and music, the fundamental frequency (F0) plays a pivotal role, defining the pitch of a sound. Pitch is the perceptual correlate of the acoustic waveform’s periodicity, and it carries tonal and rhythmic information in speech. F0 is crucial for emphasizing linguistic goals in speech, impacting gender perception, and conveying emotions.\n\n\n\nHarmonics are multiples of the fundamental frequency and contribute to the richness and quality of a sound. Frequency measures the number of oscillations occurring per second and is quantified in hertz (Hz), while amplitude denotes the power or loudness of a sound, measured in decibels (dB). Humans can typically hear sounds ranging from 0 to 140 dB, with 0 dB representing the hearing threshold for the human ear. Jitter and shimmer measurements capture frequency and amplitude instability, respectively, making them valuable tools in describing vocal characteristics, speaker evaluation, stress and emotion classification, and vocal pathology detection.\n\n\n\nAs previously said, voice is nothing else than a complex periodic sound. However, it involves a complex series of events. Imagine uttering a simple greeting like “Hello” to a friend. To do so, your brain initiates the release of air from your lungs, causing your vocal folds to vibrate. The distinctive sound of your “Hello” is a product of the precise configuration of your articulatory organs, including your teeth, tongue, oral and nasal cavities. These unique vocal signatures are influenced by factors such as gender and body size.\nFor instance, women typically have higher-pitched voices due to shorter vocal fold lengths, while larger individuals may possess lower-pitched voices due to longer vocal tracts. These vocal variations carry evolutionary implications, shaping perceptions of attractiveness, dominance, and social status.\n\n\n\nSound waves can be captured and recorded, resulting in an audio signal. During the recording process, audio is converted from its analog form, where it exists as continuous or semi-continuous changes in the signal, into digital format. This conversion is essential for saving audio as digital files. Digital audio quality is determined by the sample rate, which defines the number of “snapshot slices” taken per second of the analog signal.\n\n\n\nAnalog signals continuously represent the signal’s magnitude, while digital signals present the magnitude only in fixed time intervals, resulting in a discrete representation. Analog signals appear as continuous graphs, whereas digital signals manifest as a step-like, stair-stepped graph.\n\n\n\nSignals are symbolic or numerical values that appear in a specific order. Signal properties can be measured in different dimensions, and that’s where voice analytics takes place.\n\n\n\nTo understand human voice, we use a four-dimensional framework composed of the following dimensions: time, amplitude, frequency, and spectrum.\n\n\nTime, as a dimension of speech, captures the rhythmic intricacies of communication. It encompasses:\n\nSpeech Duration: The temporal extent of speech, revealing emotions and engagement. Extended speech duration may signal excitement or stress.\nSpeech Rate: The pace at which words are spoken, influencing persuasive communication. A faster speech rate can enhance persuasiveness.\nVoice Breaks: Moments of interruption in speech, linked to extroversion and competence.\n\n\n\n\nAmplitude, or loudness, speaks volumes about your demeanor. Uncover how sound intensity signifies dominance, aggression, extraversion, or vulnerability.\n\n\n\nPitch, a crucial aspect of speech, varies with emotions. Higher pitch correlates with anger, fear, or happiness, while lower pitch is linked to sadness or tenderness. Personality traits also affect pitch, with extroverts demonstrating higher pitch variability.\n\n\n\nThis dimension scrutinizes vocal perturbations, including vocal jitter, shimmer, harmonics-to-noise ratio (HNR), and entropy. These factors unveil emotional states, with high jitter and shimmer indicating stress, while HNR and entropy are associated with pleasure or negative moods.\n\n\n\n\n\nIn the Voice Analytics Hub, our primary tool of choice is R for voice analytics.\n\n\nDive into the world of R, the tool of choice for voice analytics. Explore how R’s capabilities in batch processing, feature extraction, comprehensive analysis, and high-quality visualizations set it apart, making it an invaluable asset in our research arsenal.\n\n\n\nEmbark on a structured journey through the voice analytics pipeline, comprising three essential phases:\n\nReading & Editing Sound Files: Witness the meticulous preparation of audio data to meet stringent research requirements.\nVisualizing Sound: Explore the power of soundwave visualization in unraveling the intricacies of human speech.\nExtracting Vocal Features: Unlock the potential of signal processing packages to extract invaluable vocal features that drive our research objectives.\n\nThese preparatory steps, though time-consuming, are the cornerstones of maintaining the integrity and quality of our voice data. They empower us to delve deep into the analysis of the captivating vocal characteristics that pique our curiosity.\nWelcome to the future of voice analytics – where the art of speech meets the science of psychology, and where R is the key to unlocking the secrets of human expression."
  },
  {
    "objectID": "basic-theory.html",
    "href": "basic-theory.html",
    "title": "The voice Analytics Hub",
    "section": "",
    "text": "Voice analytics is a multidisciplinary exploration of human speech, seamlessly integrating phonetics, acoustics, and cognitive psychology. This cutting-edge field unravels the intricacies of speech formation, transmission, and perception, offering profound insights into human communication.\nBefore diving into the world of voice and audio analytics, it’s essential to grasp the fundamental principles of acoustics. While acoustics is a vast field, we’ll cover some key concepts to provide a foundational understanding of this fascinating domain. These insights will not only enhance your comprehension of voice analytics but also empower you to better interpret your analysis results.\n\n\n\n\nSound, at its core, is a wave in motion—a concept that traces its roots to the study of water waves. These sound waves travel through various elastic media, including air, steel, and water, inducing changes in pressure and motion. Elastic media have the remarkable ability to reshape themselves when subjected to external forces.\n\n\n\nWaves can be categorized based on the direction of their movement, resulting in three primary types: transverse, longitudinal, and surface waves.\n\nTransverse Waves: In transverse waves, the particles in the medium move in a direction perpendicular to the wave’s propagation direction.\nLongitudinal Waves: Longitudinal waves involve particles moving parallel to the direction of the wave’s travel.\nSurface Waves: These waves exhibit circular particle motion, typically occurring at the interface between two media.\n\nIn the ideal fluid medium, such as air and other gases, sound is transmitted as longitudinal waves. However, in solid mediums like steel, sound can travel as both longitudinal and transverse waves.\n\n\n\nVoice and musical instruments produce complex periodic sounds, characterized by two key attributes: periodicity and complexity. Periodicity refers to the regular repetition of sound patterns, while complexity indicates the presence of multiple signal components, known as harmonic frequencies. In voice and music, the fundamental frequency (F0) plays a pivotal role, defining the pitch of a sound. Pitch is the perceptual correlate of the acoustic waveform’s periodicity, and it carries tonal and rhythmic information in speech. F0 is crucial for emphasizing linguistic goals in speech, impacting gender perception, and conveying emotions.\n\n\n\nHarmonics are multiples of the fundamental frequency and contribute to the richness and quality of a sound. Frequency measures the number of oscillations occurring per second and is quantified in hertz (Hz), while amplitude denotes the power or loudness of a sound, measured in decibels (dB). Humans can typically hear sounds ranging from 0 to 140 dB, with 0 dB representing the hearing threshold for the human ear. Jitter and shimmer measurements capture frequency and amplitude instability, respectively, making them valuable tools in describing vocal characteristics, speaker evaluation, stress and emotion classification, and vocal pathology detection.\n\n\n\nAs previously said, voice is nothing else than a complex periodic sound. However, it involves a complex series of events. Imagine uttering a simple greeting like “Hello” to a friend. To do so, your brain initiates the release of air from your lungs, causing your vocal folds to vibrate. The distinctive sound of your “Hello” is a product of the precise configuration of your articulatory organs, including your teeth, tongue, oral and nasal cavities. These unique vocal signatures are influenced by factors such as gender and body size.\nFor instance, women typically have higher-pitched voices due to shorter vocal fold lengths, while larger individuals may possess lower-pitched voices due to longer vocal tracts. These vocal variations carry evolutionary implications, shaping perceptions of attractiveness, dominance, and social status.\n\n\n\nSound waves can be captured and recorded, resulting in an audio signal. During the recording process, audio is converted from its analog form, where it exists as continuous or semi-continuous changes in the signal, into digital format. This conversion is essential for saving audio as digital files. Digital audio quality is determined by the sample rate, which defines the number of “snapshot slices” taken per second of the analog signal.\n\n\n\nAnalog signals continuously represent the signal’s magnitude, while digital signals present the magnitude only in fixed time intervals, resulting in a discrete representation. Analog signals appear as continuous graphs, whereas digital signals manifest as a step-like, stair-stepped graph.\n\n\n\nSignals are symbolic or numerical values that appear in a specific order. Signal properties can be measured in different dimensions, and that’s where voice analytics takes place.\n\n\n\nTo understand human voice, we use a four-dimensional framework composed of the following dimensions: time, amplitude, frequency, and spectrum.\n\n\nThe first dimension, time, explores the duration of soundwaves, measured in seconds or milliseconds. It helps us understand a speaker’s emotions and traits through metrics like duration of an utterance, speech rate, and voice breaks. Longer durations often indicate excitement, while faster speech rates enhance persuasiveness and show positive attributes. Voice breaks reveal introverted or less competent speakers.\n\n\n\nIn the amplitude dimension, we look at the loudness of soundwaves, measured in decibels (dB). Greater loudness suggests dominance, aggression, and extroversion. Lower loudness may indicate fear, sadness, or tenderness. Variability in loudness reflects emotional states, with high variability tied to high arousal emotions.\n\n\n\nFrequency measures how “deep” or “shrill” a sound is. Lower pitch signifies competence, trustworthiness, and persuasiveness, while higher pitch signals emotions like anger, fear, and happiness. Gender and personality traits also affect pitch.\n\n\n\nThe fourth dimension assesses vocal perturbations. Vocal jitter reflects pitch irregularities and correlates with various emotions, including stress and happiness. Vocal shimmer measures amplitude variations and signals emotions like stress and joy. Harmonics-to-noise ratio (HNR) and spectral entropy reveal feelings of pleasure, interest, and confidence, or lust and disarray, respectively.\nThese dimensions, when combined with listener perceptions, provide valuable insights into a speaker’s emotional state and personality traits. Understanding these dimensions enhances our grasp of human speech and the psychology behind it.\n\n\n\n\n\nAt the Voice Analytics Hub, our tool of preference for voice analytics is R and, thus, all the tutorials and tools we provide focus on this language.\n\n\nR is a versatile and powerful programming language and environment for statistical computing and data analysis, making it a valuable tool for sound analytics in several ways:\n\nOpen Source: R is open-source software, which means it’s freely available to anyone. This accessibility encourages collaboration, allows for continual improvement, and reduces costs associated with proprietary software.\nRich Ecosystem: R boasts a vast ecosystem of packages and libraries specifically designed for various data analysis tasks, including sound analytics. These packages provide functions and tools that streamline data processing, statistical analysis, and visualization, saving time and effort.\nStatistical Capabilities: R is renowned for its statistical capabilities, making it an ideal choice for analyzing sound data. Researchers can apply a wide range of statistical tests and models to explore patterns, trends, and relationships within sound datasets.\nData Visualization: R excels at data visualization. It offers numerous packages (e.g., ggplot2) for creating publication-quality plots and visualizations, allowing researchers to effectively communicate their findings. Newspapers like The Economist or The Washington Post usually use R to create their nice looking plots.\nFlexibility: R’s flexibility enables users to adapt it to their specific needs. Researchers can write custom functions and scripts tailored to their sound analytics tasks, offering greater control and customization.\nCommunity Support: R has a large and active user community, which means that help is readily available through forums, mailing lists, and online resources. This support network can be invaluable when encountering challenges during sound analytics projects.\nIntegration: R can seamlessly integrate with other programming languages and tools, facilitating the incorporation of sound analytics into broader data analysis workflows. For example, R can be used in conjunction with Python for machine learning tasks.\nReproducibility: R’s script-based approach ensures that analyses are fully documented and reproducible. This is essential for maintaining transparency and rigor in sound analytics research.\nCross-Platform Compatibility: R is available for multiple operating systems, including Windows, macOS, and Linux, ensuring compatibility with various computing environments.\nCommunity Contributions: The R community actively develops and contributes new packages and functionalities. This means that the toolset for sound analytics in R is continually evolving and expanding.\n\nIn summary, R’s open-source nature, statistical prowess, data visualization capabilities, flexibility, and strong community support make it a compelling choice for sound analytics. Researchers can leverage R’s extensive ecosystem to effectively analyze, visualize, and draw insights from sound data for a wide range of applications, including speech analysis, audio processing, and more."
  },
  {
    "objectID": "voiceR.html",
    "href": "voiceR.html",
    "title": "The voice Analytics Hub",
    "section": "",
    "text": "voiceR is an R package that simplifies and largely automates practical voice analytics for social science research. This package offers an accessible and easy-to-use interface, including an interactive Shiny app, that simplifies the processing, extraction, analysis, and reporting of voice recording data in the behavioral and social sciences. The package includes batch processing capabilities to read and analyze multiple voice files in parallel, automates the extraction of key vocal features for further analysis, and automatically generates APA formatted reports for typical between-group comparisons in experimental social science research.\n\n\n\n\n\n\n\n\n\n\n\n\nVideo\n\n\n\nGetting started with voiceR is a breeze. Simply install the package directly from CRAN using the install.packages(\"voiceR\") command. Once installed, you’re ready to embark on your voice analytics journey with voiceR.\n\n\n\nTo ensure seamless processing, voiceR relies on a specific file naming convention. Audio file names should adhere to a consistent format, with components like ID, condition, and dimension separated by designated symbols (mainly non alphanumeric symbols, such as underscore). This structure ensures efficient data management and analysis. While order and presence of components can vary, consistency within a session is crucial for optimal performance.\n\n\n\nExamples of File Name Patterns\n\n\n\n\n\nvoiceR offers a suite of functions designed to simplify the voice analytics process. These functions cover reading and preprocessing audio files, automatic feature extraction, visualization of results, and even automatic report generation.\n\nReading Multiple Audio Files: Start by using the readAudio() function to systematically read audio files in your chosen directory. Customize the process with optional filters for specific file patterns.\nPreprocessing Multiple Audio Files: After importing audio files, preprocess them effortlessly using the preprocess() function. Normalize amplitude and remove background noise with ease.\nAutomatic Feature Extraction: Extract vital vocal features from your audio files automatically using the autoExtract() function. Choose between analyzing files directly from a directory or using pre-imported audio files.\nVisualizing Results: Gain insights into your data with the normalityPlots() and comparisonPlots() functions. Visualize audio features’ distribution and differences across conditions or dimensions.\nAutomatic Report Generation: Ensure thorough documentation of your findings with the autoReport() function. Generate HTML reports with APA-formatted text, tables, and visualizations.\n\n\n\n\nFor the ultimate user-friendly experience, voiceR offers the voiceRApp() function. Launch the voiceR Shiny app to select and analyze multiple audio files effortlessly. Explore dynamic results and download comprehensive reports without writing a single line of code.\n\n\n\nFor a detailed understanding of voiceR’s capabilities and functionalities, consult the package documentation here. Discover how voiceR can elevate your voice analytics endeavors and make exploring voice data accessible and engaging. Your journey into voice analytics starts here with voiceR."
  },
  {
    "objectID": "basic-resources.html",
    "href": "basic-resources.html",
    "title": "The voice Analytics Hub",
    "section": "",
    "text": "Welcome to our curated collection of resources for those interested in diving deeper into the world of voice analytics. Whether you’re a beginner looking to understand the basics or an experienced analyst seeking advanced knowledge, these resources will help you explore the fascinating field of voice analytics.\n\n\n\n\n\nvoiceR: simplifies and largely automates practical voice analytics for social science research [link]\nseewave: functions for analysing, manipulating, displaying, editing and synthesizing time waves (particularly sound). [link]\ntuneR: analyze music and speech, extract features like MFCCs, handle wave files and their representation in various ways, read mp3, read midi, perform steps of a transcription, etc. [link]\nphonTools: tools for the organization, display, and analysis of the sorts of data frequently encountered in phonetics research and experimentation. [link]\nsoundgen: open-source toolbox for voice synthesis, manipulation, and analysis. [link]\n\n\n\n\n\nHildebrand, C., Efthymiou, F., Busquet, F., Hampton, W. H., Hoffman, D. L., & Novak, T. P. (2020). Voice analytics in business research: Conceptual foundations, acoustic feature extraction, and applications. Journal of Business Research, 121, 364-374. [link]\n\n\n\n\n\nLaver, J. (1991). The Gift of Speech: Papers in the Analysis of Speech and Voice."
  }
]